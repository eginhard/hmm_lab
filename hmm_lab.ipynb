{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Hidden Markov Models\n",
    "====================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulas and definitions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   A **Markov chain** or **process** is a sequence of events, usually called\n",
    "    **states**, the probability of each of which is dependent only on the\n",
    "    event immediately preceding it.\n",
    "-   A **hidden Markov model** (HMM) represents stochastic sequences as\n",
    "    Markov chains where the states are not directly observed, but are\n",
    "    associated with a probability density function (pdf). The generation\n",
    "    of a random sequence is then the result of a random walk in the chain\n",
    "    (i.e. the browsing of a random sequence of states\n",
    "    $Q=\\{q_1,\\cdots q_T\\}$) and of a draw (called an *emission*) at each\n",
    "    visit of a state.  \n",
    "    The sequence of states, which is the quantity of interest in speech\n",
    "    recognition and in most of the other pattern recognition problems, can\n",
    "    be observed only *through* the stochastic processes defined into each\n",
    "    state (i.e. you must know the parameters of the pdfs of each state\n",
    "    before being able to associate a sequence of states\n",
    "    $Q=\\{q_1,\\cdots q_T\\}$ to a sequence of observations\n",
    "    $X=\\{x_1,\\cdots x_T\\}$). The true sequence of states is therefore\n",
    "    *hidden* by a first layer of stochastic processes. HMMs are\n",
    "    *dynamic models*, in the sense that they are specifically designed to\n",
    "    account for some macroscopic structure of the random sequences. In the\n",
    "    previous lab, concerned with *Gaussian Statistics and Statistical\n",
    "    Pattern Recognition*, random sequences of observations were considered\n",
    "    as the result of a series of *independent* draws in one or several\n",
    "    Gaussian densities. To this simple statistical modeling scheme, HMMs\n",
    "    add the specification of some *statistical dependence* between the\n",
    "    (Gaussian) densities from which the observations are drawn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters of a HMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The **emission probabilities** are the pdfs that characterize each state\n",
    "    $q_i$, i.e. $p(x|q_i)$. To simplify the notations, they will be\n",
    "    denoted $b_i(x)$. For practical reasons, they are usually Gaussian or\n",
    "    mixtures of Gaussians, but the states could be parameterized in\n",
    "    terms of any other kind of pdf (including discrete probabilities and\n",
    "    artificial neural networks).\n",
    "-   The **transition probabilities** are the probability to go from a state\n",
    "    $i$ to a state $j$, i.e. $P(q_j|q_i)$. They are stored in matrices\n",
    "    where each term $a_{i,j}$ denotes a probability $P(q_j|q_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-emitting initial and final states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a random sequence $X=\\{x_1,\\cdots x_T\\}$ has a finite length $T$, the\n",
    "fact that the sequence begins or ends has to be modeled as two\n",
    "additional discrete events. In HMMs, this corresponds to the addition of\n",
    "two *non-emitting states*, the initial state and the final state. Since\n",
    "their role is just to model the *start* or *end* events, they are not\n",
    "associated with any emission probabilities.  \n",
    "The transitions starting from the initial state correspond to the\n",
    "modeling of an *initial state distribution* $P(I|q_j)$, which indicates\n",
    "the probability to start the state sequence with the emitting state\n",
    "$q_j$.  \n",
    "The final state usually has only one non-null transition that loops onto\n",
    "itself with a probability of $1$ (it is an *absorbent state*), so that\n",
    "the state sequence gets \"trapped\" into it when it is reached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ergodic versus left-right HMMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A HMM allowing for transitions from any emitting state to any other\n",
    "emitting state is called an **ergodic HMM**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import hmm1, hmm2, hmm3, hmm4, hmm5, hmm6\n",
    "\n",
    "hmm1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, an HMM where the transitions only go from one state to\n",
    "itself or to a unique follower is called a **left-right HMM**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm3.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Values used throughout the experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 2-dimensional Gaussian densities will be used to model\n",
    "simulated vowel observations, where the considered features are the two\n",
    "first formants. They will be combined into Markov Models that will be\n",
    "used to model some observation sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data import GAUSSIANS\n",
    "for vowel, gaussian in GAUSSIANS.items():\n",
    "    print(f\"Gaussian('{vowel}'): mean={gaussian.mean.tolist()}, \"\n",
    "          f\"cov={gaussian.cov.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the resulting HMMs. We represent the initial state $I$ and\n",
    "final state $F$ with mean and variance of zero, but they don't actually\n",
    "emit any observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hmm1.labels)\n",
    "print(hmm1.transitions)\n",
    "for gaussian in hmm1.gaussians:\n",
    "    print(f\"mean={gaussian.mean.tolist()}, cov={gaussian.cov.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise the transition matrix as a graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the remaining HMMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm2.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm3.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm4.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm5.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm6.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating samples from HMMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a sample $X$ coming from the Hidden Markov Models `hmm1`,\n",
    "`hmm2`, `hmm3` and `hmm4`. Use the `HMM.sample()` method to do several\n",
    "draws with each of these models and plot them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import HMM\n",
    "help(HMM.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a sample and plot the resulting sequence. The sequence is\n",
    "represented by a gray line where each point is overlaid with a colored\n",
    "dot. The different colors indicate the state from which any particular\n",
    "dot has been drawn.\n",
    "\n",
    "The lefthand plots highlight the notion of a sequence of states\n",
    "associated with a sequence of observations. The 2-dimensional righthand\n",
    "plot highlights the spatial distribution of the observations and also\n",
    "shows the Gaussian distributions from which the samples for each state\n",
    "were drawn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, states, labels = hmm1.sample(plot=True)\n",
    "print(X)\n",
    "print(states)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this several times and also draw samples from the other models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, states, labels = hmm4.sample(plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  How can you verify that a transition matrix is valid?\n",
    "2.  What is the effect of the different transition matrices on the\n",
    "    sequences obtained during the current experiment? Hence, what is the\n",
    "    role of the transition probabilities in the Markovian modeling\n",
    "    framework?\n",
    "3.  What would happen if we didn't have a final state ?\n",
    "4.  In the case of HMMs with plain Gaussian emission probabilities, what\n",
    "    quantities should be present in the complete parameter set $\\Theta$\n",
    "    that specifies a particular model?  \n",
    "    If the model is ergodic with $N$ states (including the initial and\n",
    "    final), and represents data of dimension $D$, what is the total\n",
    "    number of parameters in $\\Theta$?\n",
    "5.  Which type of HMM (ergodic or left-right) would you use to model\n",
    "    words?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern recognition with HMMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood of an observation sequence given a HMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we have generated some stochastic observation sequences\n",
    "from various HMMs. Now, it is useful to study the reverse problem,\n",
    "namely: given a new observation sequence and a set of models, which\n",
    "model explains best the sequence, or in other terms which model gives\n",
    "the highest likelihood to the data?\n",
    "\n",
    "To solve this problem, it is necessary to compute $p(X|\\Theta)$,\n",
    "i.e. the likelihood of an observation sequence given a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of a state sequence $Q$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a state sequence $Q=\\{q_1,\\cdots,q_T\\}$ coming from a\n",
    "HMM with parameters $\\Theta$ corresponds to the product of the\n",
    "transition probabilities from one state to the following:\n",
    "\n",
    "$$\n",
    "P(Q|\\Theta) = \\prod_{t=1}^{T-1} a_{t,t+1}\n",
    "= a_{1,2} \\cdot a_{2,3} \\cdots a_{T-1,T}\n",
    "$$\n",
    "\n",
    "\n",
    "In practice we will do the computations in log space to avoid numerical\n",
    "underflow:\n",
    "\n",
    "$$\n",
    "\\log P(Q|\\Theta) = \\sum_{t=1}^{T-1} \\log a_{t,t+1}\n",
    "= \\log a_{1,2} + \\log a_{2,3} \\cdots \\log a_{T-1,T}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = hmm3\n",
    "X, states, labels = hmm.sample()\n",
    "print(\"States:\", states)\n",
    "\n",
    "from_states = states[:-1]  # Row indices into hmm3.transitions\n",
    "to_states = states[1:]     # Column indices\n",
    "log_a = hmm.log_transitions[from_states, to_states]\n",
    "print(\"Transition log probs:\", log_a)\n",
    "\n",
    "log_P_Q = sum(log_a)\n",
    "print(\"log P(Q):\", log_P_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood of an observation sequence $X$ given a path $Q$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an observation sequence $X=\\{x_1,x_2,\\cdots,x_T\\}$ and a state\n",
    "sequence $Q=\\{q_1,\\cdots,q_T\\}$ (of the same length) determined from a\n",
    "HMM with parameters $\\Theta$, the likelihood of $X$ along the path $Q$\n",
    "is equal to:\n",
    "\n",
    "$$\n",
    "p(X|Q,\\Theta) = \\prod_{i=1}^T p(x_i|q_i,\\Theta)\n",
    "= b_1(x_1) \\cdot b_2(x_2) \\cdots b_T(x_T)\n",
    "$$\n",
    "\n",
    "\n",
    "i.e. it is the product of the emission probabilities computed along\n",
    "the considered path.\n",
    "\n",
    "In the previous lab, we had learned how to compute the likelihood of a\n",
    "single observation with respect to a Gaussian model. This method can be\n",
    "applied here, for each term $x_i$, if the states contain Gaussian pdfs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_X_given_Q = sum(np.log(hmm.gaussians[state].pdf(x))\n",
    "                      for x, state in zip(X, states[1:-1]))\n",
    "\n",
    "print(\"log p(X|Q):\", log_p_X_given_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint likelihood of an observation sequence $X$ and a path $Q$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that $X$ and $Q$ occur simultaneously, $p(X,Q|\\Theta)$,\n",
    "decomposes into a product of the two quantities defined previously:\n",
    "\n",
    "$$\n",
    "p(X,Q|\\Theta) = p(X|Q,\\Theta) P(Q|\\Theta)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"log p(X,Q):\", log_p_X_given_Q + log_P_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood of observations with respect to a HMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of an observation sequence $X=\\{x_1,x_2,\\cdots,x_T\\}$\n",
    "with respect to a Hidden Markov Model with parameters $\\Theta$ expands\n",
    "as follows:\n",
    "\n",
    "$$\n",
    "    p(X|\\Theta) = \\sum_{every~possible~Q} p(X,Q|\\Theta)\n",
    "$$\n",
    "\n",
    "\n",
    "i.e. it is the sum of the joint likelihoods of the sequence over all\n",
    "possible state sequence allowed by the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Forward Algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the enumeration of every possible state sequence is\n",
    "infeasible even for small values of $N$ and $T$. Nevertheless,\n",
    "$p(X|\\Theta)$ can be computed in a recursive way (dynamic programming)\n",
    "by the **forward algorithm**. This algorithm defines a forward variable\n",
    "$\\alpha_t(i)$ corresponding to:\n",
    "\n",
    "$$\n",
    "    \\alpha_t(i) = p(x_1,x_2,\\cdots x_t,q^t=q_i|\\Theta)\n",
    "$$\n",
    "\n",
    "\n",
    "i.e. $\\alpha_t(i)$ is the probability of having observed the partial\n",
    "sequence $\\{x_1,x_2,\\cdots,x_t\\}$ *and* being in the state $i$ at time\n",
    "$t$ (event denoted $q_i^t$ in the course), given the parameters\n",
    "$\\Theta$. For a HMM with $N$ states (where states 1 and $N$ are the\n",
    "non-emitting initial and final states, and states $2 \\cdots N-1$ are\n",
    "emitting), $\\alpha_t(i)$ can be computed recursively as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initialization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "        \\alpha_1(i) = a_{1,i} \\cdot b_i(x_1), \\;\\;\\;\\; 2 \\leq i \\leq N-1\n",
    "$$\n",
    "\n",
    "where $a_{1,i}$ are the transitions from the initial non-emitting state\n",
    "to the emitting states with pdfs $b_{i,\\,i = 2 \\cdots N-1}(x)$. Note\n",
    "that $b_1(x)$ and $b_N{x}$ do not exist since they correspond to the\n",
    "non-emitting initial and final states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import X1, X2, X3, X4, X5, X6\n",
    "\n",
    "# Let's pick a fixed sequence defined in `data.py` to make\n",
    "# the results reproducible\n",
    "hmm = hmm3\n",
    "X = X2\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First precompute the b(x), i.e. pdfs, for all observations\n",
    "# and emitting states\n",
    "log_bs = np.zeros((len(X), hmm.n_states))\n",
    "for state in range(1, hmm.n_states - 1):\n",
    "    log_bs[:,state] = np.log(hmm.gaussians[state].pdf(X))\n",
    "print(log_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then compute the initial alphas\n",
    "alphas = np.ones((len(X), hmm.n_states)) * -np.inf\n",
    "alphas[0] = hmm.log_transitions[0] + log_bs[0]\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Recursion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "        \\alpha_{t+1}(j) = \\left[ \\sum_{i=2}^{N-1} \\alpha_{t}(i) \\cdot a_{i,j} \\right] b_j(x_{t+1}),\n",
    "        \\;\\;\\;\\; \\begin{array}{l} 1 \\leq t \\leq T \\\\ 2 \\leq j \\leq N-1 \\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will show how to compute the alphas in 2 different ways\n",
    "alphas_2 = alphas.copy()\n",
    "\n",
    "# Basic Python way with 3 for-loops\n",
    "for t in range(1, len(X)):\n",
    "    for j in range(1, hmm.n_states - 1):\n",
    "        log_as = -np.inf\n",
    "        for i in range(1, hmm.n_states - 1):\n",
    "            log_as = np.logaddexp(\n",
    "                log_as, alphas[t-1, i] + hmm.log_transitions[i, j])\n",
    "        alphas[t, j] = log_as + log_bs[t, j]\n",
    "\n",
    "# Remove the innermost loop thanks to Numpy\n",
    "for t in range(1, len(X)):\n",
    "    for j in range(1, hmm.n_states - 1):\n",
    "        alphas_2[t, j] = np.logaddexp.reduce(\n",
    "            alphas_2[t-1] + hmm.log_transitions[:, j]) + log_bs[t, j]\n",
    "\n",
    "# Check that they are indeed the same\n",
    "print(alphas)\n",
    "assert np.allclose(alphas, alphas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Termination\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "        p(X|\\Theta) = \\left[ \\sum_{i=2}^{N-1} \\alpha_{T}(i) \\cdot a_{i,N} \\right]\n",
    "$$\n",
    "\n",
    "i.e. at the end of the observation sequence, sum the probabilities of\n",
    "the paths converging to the final state $N$. (For more detail about the\n",
    "forward procedure, refer to Lawrence Rabiner's\n",
    "[Tutorial on Hidden Markov\n",
    "Models and Selected Applications in Speech Recognition](http://web.mit.edu/6.435/www/Rabiner89.pdf)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_X = np.logaddexp.reduce(\n",
    "    alphas[-1] + hmm.log_transitions[:, hmm.n_states - 1])\n",
    "print(\"p(X|hmm):\", log_p_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure raises a very important implementation issue. As a matter\n",
    "of fact, the computation of the $\\alpha_t$ vector consists in products\n",
    "of a large number of values that are less than 1 (in general,\n",
    "*significantly* less than 1). Hence, after a few observations\n",
    "($t \\approx$ 10), the values of $\\alpha_t$ head exponentially to 0, and\n",
    "the floating point arithmetic precision is exceeded (even in the case of\n",
    "double precision arithmetics). Two solutions exist for that problem. One\n",
    "consists in scaling the values and undo the scaling at the end of the\n",
    "procedure: see Rabiner's tutorial for more explanations. The other\n",
    "solution consists in using log-likelihoods and log-probabilities, and to\n",
    "compute $\\log p(X|\\Theta)$ instead of $p(X|\\Theta)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  The following formula can be used to compute the log of a sum given\n",
    "    the logs of the sum's arguments:\n",
    "    \n",
    "    $$\n",
    "       \\log(a+b) = f(\\log a,\\log b) = \\log a + \\log \\left( 1 + e^{(\\log b - \\log a)} \\right)\n",
    "       $$\n",
    "    \n",
    "    Prove its validity.\n",
    "    \n",
    "    Naturally, one has the choice between using\n",
    "    $\\log(a+b) = \\log a + \\log \\left( 1 + e^{(\\log b - \\log a)} \\right)$\n",
    "    or\n",
    "    $\\log(a+b) = \\log b + \\log \\left( 1 + e^{(\\log a - \\log b)} \\right)$,\n",
    "    which are equivalent in theory. If $\\log a > \\log b$, which version\n",
    "    leads to the most precise implementation?\n",
    "\n",
    "2.  Express the log version of the forward recursion. (Don't fully\n",
    "    develop the log of the sum in the recursion step, just call it\n",
    "    \"logsum\":\n",
    "    $\\sum_{i=1}^{N} x_i \\stackrel{\\log}{\\longmapsto} \\mbox{logsum}_{i=1}^{N} ( \\log x_i )$.)\n",
    "    In addition to the arithmetic precision issues, what are the other\n",
    "    computational advantages of the log version?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward recursion allows us to compute the likelihood of an\n",
    "observation sequence with respect to a HMM. Hence, given a sequence of\n",
    "features, we are able to find the most likely generative model in a\n",
    "Maximum Likelihood sense. What additional quantities and assumptions do\n",
    "we need to perform a true Bayesian classification rather than a Maximum\n",
    "Likelihood classification of the sequences?\n",
    "\n",
    "Which additional condition makes the result of Bayesian classification\n",
    "equivalent to the result of ML classification?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, for speech recognition, it is very often assumed that all\n",
    "the model priors are equal (i.e. that the words or phonemes to recognize\n",
    "have equal probabilities of occurring in the observed speech). Hence,\n",
    "the speech recognition task consists mostly in performing the Maximum\n",
    "Likelihood classification of acoustic feature sequences. For that\n",
    "purpose, we must have of a set of HMMs that model the acoustic sequences\n",
    "corresponding to a set of phonemes or a set of words. These models can\n",
    "be considered as \"stochastic templates\". Then, we associate a new\n",
    "sequence to the most likely generative model. This part is called the\n",
    "**decoding** of the acoustic feature sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the sequences `X1`, `X2`, &#x2026;, `X6`, given in the file\n",
    "`data.py`, in a maximum likelihood sense with respect to the six Markov\n",
    "models defined above. Use the method `HMM.forward(X)` to compute the\n",
    "log-forward recursion expressed in the previous section. Store the\n",
    "results in the array `log_prob` (they will be used in the next section)\n",
    "and note them in the table below.\n",
    "\n",
    "| Sequence|$\\log p(X\\vert\\Theta_1)$|$\\log p(X\\vert\\Theta_2)$|$\\log p(X\\vert\\Theta_3)$|$\\log p(X\\vert  \\Theta_4)$|$\\log p(X\\vert  \\Theta_5)$|$\\log p(X\\vert  \\Theta_6)$|Most likely model|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "| $X1$||||||||\n",
    "| $X2$||||||||\n",
    "| $X3$||||||||\n",
    "| $X4$||||||||\n",
    "| $X5$||||||||\n",
    "| $X6$||||||||\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm1.plot_sample(X1)\n",
    "hmm1.forward(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the `log_prob` array can be done automatically with the help of\n",
    "loops:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = np.zeros((6, 6))\n",
    "for i, X in enumerate([X1, X2, X3, X4, X5, X6]):\n",
    "    for j, hmm in enumerate([hmm1, hmm2, hmm3, hmm4, hmm5, hmm6]):\n",
    "        log_prob[i, j] = hmm.forward(X)\n",
    "\n",
    "print(log_prob.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal state sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In speech recognition and several other pattern recognition\n",
    "applications, it is useful to associate an \"optimal\" sequence of states\n",
    "to a sequence of observations, given the parameters of a model. For\n",
    "instance, in the case of speech recognition, knowing which frames of\n",
    "features \"belong\" to which state allows to locate the word boundaries\n",
    "across time. This is called the *alignment* of acoustic feature\n",
    "sequences.\n",
    "\n",
    "A \"reasonable\" optimality criterion consists in choosing the state\n",
    "sequence (or *path*) that has the maximum likelihood with respect to a\n",
    "given model. This sequence can be determined recursively via the\n",
    "**Viterbi algorithm**. This algorithm makes use of two variables:\n",
    "\n",
    "-   The *highest* likelihood $\\delta_t(i)$ along a *single* path among\n",
    "    all the paths ending in state $i$ at time $t$:\n",
    "\n",
    "$$\n",
    "\\delta_t(i) = \\max_{q_1,q_2,\\cdots,q_{t-1}}\n",
    "p(q_1,q_2,\\cdots,q_{t-1},q^t=q_i,x_1,x_2,\\cdots x_t|\\Theta)\n",
    "$$\n",
    "\n",
    "-   A variable $\\psi_t(i)$ which allows to keep track of the \"best path\"\n",
    "    ending in state $i$ at time $t$:\n",
    "\n",
    "$$\n",
    "\\psi_t(i) = \\mbox{arg}\\max_{\\hspace{-4.5ex}q_1,q_2,\\cdots,q_{t-1}}\n",
    "p(q_1,q_2,\\cdots,q_{t-1},q^t=q_i,x_1,x_2,\\cdots x_t|\\Theta)\n",
    "$$\n",
    "\n",
    "Note that these variables are vectors of $(N-2)$ elements, $(N-2)$ being\n",
    "the number of emitting states. With the help of these variables, the\n",
    "algorithm takes the following steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initialization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\delta_1(i) &= a_{1,i} \\cdot b_i(x_1), \\;\\;\\;\\; 2 \\leq i \\leq N-1 \\\\\n",
    "    \\psi_1(i) &= 0\n",
    "\\end{align*}\n",
    "\n",
    "where, again, $a_{1,i}$ are the transitions from the initial non-emitting\n",
    "state to the emitting states with pdfs $b_{i,\\,i = 2 \\cdots N-1}(x)$,\n",
    "and where $b_1(x)$ and $b_N{x}$ do not exist since they correspond to\n",
    "the non-emitting initial and final states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = hmm3\n",
    "X = X2\n",
    "\n",
    "# First precompute the b(x), i.e. pdfs, for all observations\n",
    "# and emitting states\n",
    "log_bs = np.zeros((len(X), hmm.n_states))\n",
    "for state in range(1, hmm.n_states - 1):\n",
    "    log_bs[:,state] = np.log(hmm.gaussians[state].pdf(X))\n",
    "print(log_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the initial deltas\n",
    "deltas = np.ones((len(X), hmm.n_states)) * -np.inf\n",
    "deltas[0] = hmm.log_transitions[0] + log_bs[0]\n",
    "print(deltas)\n",
    "\n",
    "# Initialize the backpointers\n",
    "pointers = np.zeros((len(X), hmm.n_states), dtype=int)\n",
    "print(pointers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Recursion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "        \\delta_{t+1}(j) &= \\max_{2 \\leq i \\leq N-1}\n",
    "            \\left[ \\delta_{t}(i) \\cdot a_{i,j} \\right]\n",
    "            \\cdot b_j(x_{t+1}),\n",
    "        \\;\\;\\;\\; \\begin{array}{l} 1 \\leq t \\leq T-1 \\\\ 2 \\leq j \\leq N-1 \\end{array}\n",
    "        \\\\\n",
    "        \\psi_{t+1}(j) &= \\mbox{arg}\\hspace{-0.5ex}\\max_{\\hspace{-3ex}2 \\leq i \\leq N-1}\n",
    "        \\left[ \\delta_{t}(i) \\cdot a_{i,j} \\right],\n",
    "        \\;\\;\\;\\; \\begin{array}{l} 1 \\leq t \\leq T-1 \\\\ 2 \\leq j \\leq N-1 \\end{array}\n",
    "\\end{align*}\n",
    "\n",
    "*Optimal policy is composed of optimal sub-policies*: find the path that\n",
    "leads to a maximum likelihood considering the best likelihood at the\n",
    "previous step and the transitions from it; then multiply by the current\n",
    "likelihood given the current state. Hence, the best path is found by\n",
    "induction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1, len(X)):\n",
    "    for j in range(1, hmm.n_states - 1):\n",
    "        deltas[t, j] = np.max(\n",
    "            deltas[t-1] + hmm.log_transitions[:, j]) + log_bs[t, j]\n",
    "        pointers[t, j] = np.argmax(\n",
    "            deltas[t-1] + hmm.log_transitions[:, j])\n",
    "\n",
    "print(deltas)\n",
    "print(pointers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Termination\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "        p^*(X|\\Theta) &= \\max_{2 \\leq i \\leq N-1}\n",
    "            \\left[ \\delta_{T}(i) \\cdot a_{i,N} \\right] \\\\\n",
    "        q_T^* &= \\mbox{arg}\\hspace{-0.5ex}\\max_{\\hspace{-3ex}2 \\leq i \\leq N-1}\n",
    "            \\left[ \\delta_{T}(i) \\cdot a_{i,N} \\right]\n",
    "\\end{align*}\n",
    "\n",
    "Find the best likelihood when the end of the observation sequence is\n",
    "reached, given that the final state is the non-emitting state $N$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_vit_X = np.max(\n",
    "    deltas[-1] + hmm.log_transitions[:, hmm.n_states - 1])\n",
    "print(\"p*(X|hmm):\", log_p_vit_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine from which state the final state was reached\n",
    "path = np.zeros((len(X)), dtype=int)\n",
    "path[-1] = np.argmax(\n",
    "    deltas[-1] + hmm.log_transitions[:, hmm.n_states - 1])\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Backtracking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "        Q^* = \\{q_1^*,\\cdots,q_T^*\\} \\;\\;\\;\\;\\mbox{so that}\\;\\;\\;\\;\n",
    "        q_t^* = \\psi_{t+1}(q_{t+1}^*), \\;\\;\\;\\; t = T-1, T-2, \\cdots, 1\n",
    "$$\n",
    "\n",
    "Read (decode) the best sequence of states from the $\\psi_t$ vectors.\n",
    "Remember that $\\psi_t (j)$ stores the state from which we came if the\n",
    "best sequence goes through state $j$ at time $t$. To get the path, we\n",
    "therefore just need to follow the backpointers in reverse order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(X) - 2, -1, -1):\n",
    "    path[t] = pointers[t + 1, path[t + 1]]\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the state sequence will always include the initial state\n",
    "at the beginning and the final state at the end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the Viterbi algorithm delivers *two* useful results, given an\n",
    "observation sequence $X=\\{x_1,\\cdots,x_T\\}$ and a model $\\Theta$:\n",
    "\n",
    "-   The selection, among all the possible paths in the considered model, of the\n",
    "    *best path* $Q^* = \\{q^*_1,\\cdots,q^*_T\\}$, which corresponds to the\n",
    "    state sequence giving a maximum of likelihood to the observation\n",
    "    sequence $X$;\n",
    "-   The *likelihood along the best path*,\n",
    "    $p(X,Q^*|\\Theta) = p^*(X|\\Theta)$. As opposed to the the forward\n",
    "    procedure, where all the possible paths are considered, the Viterbi\n",
    "    computes a likelihood along the best path only.\n",
    "\n",
    "(For more detail about the Viterbi algorithm, refer to Lawrence\n",
    "Rabiner's [Tutorial on\n",
    "Hidden Markov Models and Selected Applications in Speech Recognition](http://web.mit.edu/6.435/www/Rabiner89.pdf)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  From an algorithmic point of view, what is the main difference\n",
    "    between the computation of the $\\delta$ variable in the Viterbi\n",
    "    algorithm and that of the $\\alpha$ variable in the forward procedure?\n",
    "2.  Give the log version of the Viterbi algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Use the function `HMM.viterbi(X)` to find the best path of the\n",
    "    sequences $X_1, \\cdots X_6$ with respect to the most likely model\n",
    "    found with the forward algorithm (i.e. $X_1$: `hmm1`, $X_2$: `hmm3`, $X_3$: `hmm5`,\n",
    "    $X_4$: `hmm4`, $X_5$: `hmm6` and $X_6$: `hmm2`). Compare with the\n",
    "    state sequences $ST_1, \\cdots ST_6$ originally used to generate\n",
    "    $X_1, \\cdots X_6$ (use the function\n",
    "    `HMM.compare_sequences(X, S1, S2)`, which provides a view of the first\n",
    "    dimension of the observations as a time series, and allows to compare\n",
    "    the original alignment to the Viterbi solution).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import ST1, ST2, ST3, ST4, ST5, ST6\n",
    "\n",
    "best_states, log_viterbi = hmm1.viterbi(X1)\n",
    "hmm1.compare_sequences(X1, ST1, best_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_states, log_viterbi = hmm3.viterbi(X2)\n",
    "hmm3.compare_sequences(X2, ST2, best_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_states, log_viterbi = hmm5.viterbi(X3)\n",
    "hmm5.compare_sequences(X3, ST3, best_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_states, log_viterbi = hmm4.viterbi(X4)\n",
    "hmm4.compare_sequences(X4, ST4, best_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_states, log_viterbi = hmm6.viterbi(X5)\n",
    "hmm6.compare_sequences(X5, ST5, best_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_states, log_viterbi = hmm2.viterbi(X6)\n",
    "hmm2.compare_sequences(X6, ST6, best_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Use the function `HMM.viterbi(X)` to compute the probabilities of the\n",
    "    sequences $X_1, \\cdots X_6$ along the best paths with respect to each\n",
    "    model $\\Theta_1, \\cdots \\Theta_6$. Note your results below. Compare\n",
    "    with the log-likelihoods obtained previously with the forward algorithm.\n",
    "\n",
    "| Sequence|$\\log p^*(X\\vert\\Theta_1)$|$\\log p^*(X\\vert\\Theta_2)$|$\\log p^*(X\\vert\\Theta_3)$|$\\log p^*(X\\vert\\Theta_4)$|$\\log p^*(X\\vert\\Theta_5)$|$\\log p^*(X\\vert\\Theta_6)$|Most likely model|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "| $X1$||||||||\n",
    "| $X2$||||||||\n",
    "| $X3$||||||||\n",
    "| $X4$||||||||\n",
    "| $X5$||||||||\n",
    "| $X6$||||||||\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_viterbi = np.zeros((6, 6))\n",
    "for i, X in enumerate([X1, X2, X3, X4, X5, X6]):\n",
    "    for j, hmm in enumerate([hmm1, hmm2, hmm3, hmm4, hmm5, hmm6]):\n",
    "        log_viterbi[i, j] = hmm.viterbi(X)[1]\n",
    "\n",
    "print(log_viterbi.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with the complete log-likelihoods from the forward algorithm\n",
    "print((log_prob - log_viterbi).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the likelihood along the best path a good approximation of the real\n",
    "likelihood of a sequence given a model ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of HMMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding or aligning acoustic feature sequences requires the prior\n",
    "specification of the parameters of some HMMs. As explained before,\n",
    "these models have the role of stochastic templates to which we compare\n",
    "the observations. But how to determine templates that represent\n",
    "efficiently the phonemes or the words that we want to model? The\n",
    "solution is to estimate the parameters of the HMMs from a database\n",
    "containing observation sequences, in a supervised or an unsupervised\n",
    "way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab session, we have learned how to estimate the\n",
    "parameters of Gaussian pdfs given a set of training data. Suppose that\n",
    "you have a database containing several utterances of the imaginary\n",
    "word / aiy / , and that you want to train a HMM for this word. Suppose also\n",
    "that this database comes with a *labeling* of the data, i.e. some data\n",
    "structures that tell you where are the phoneme boundaries for each\n",
    "instance of the word.\n",
    "\n",
    "1.  Which model architecture (ergodic or left-right)\n",
    "    would you choose? With how many states? Justify your choice.\n",
    "2.  How would you compute the parameters of the proposed HMM?\n",
    "3.  Suppose you didn't have the phonetic labeling (i.e. you do\n",
    "    *unsupervised training*). Propose a recursive procedure to train\n",
    "    the model, making use of one of the algorithms studied during the\n",
    "    present session.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab was originally developed by Sacha Krstulović, Hervé\n",
    "Bourlard, Hemant Misra, and Mathew Magimai-Doss for the *Speech Processing and\n",
    "Speech Recognition* course at École polytechnique fédérale de Lausanne (EPFL).\n",
    "The original Matlab version is available here:\n",
    "[http://publications.idiap.ch/index.php/publications/show/739](http://publications.idiap.ch/index.php/publications/show/739)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
